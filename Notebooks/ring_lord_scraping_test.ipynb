{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ring Lord Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from time import sleep\n",
    "import datetime\n",
    "\n",
    "# Selenium Scraping imports\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver import Chrome \n",
    "from selenium.webdriver.chrome.service import Service \n",
    "from selenium.webdriver.common.by import By \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set Variables\n",
    "SLEEP_TIME = 5\n",
    "STARTING_LINK = 'https://theringlord.com/rings/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find All Product Pages\n",
    "\n",
    "The layout of The Ring Lord's website can be described as a series of collection and product pages where a collection page's links of interest are to other collection pages or product pages. As we start on a collection page and the number of collection pages to reach a prodcut page is not fixed but not too high I feel that recursion is the most reasonable strategy to find all the product pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to find all the product pages\n",
    "def page_parser(url: str) -> None:\n",
    "    # Ensure responsibility interval is respected\n",
    "    sleep(SLEEP_TIME)\n",
    "\n",
    "    # Print basic info\n",
    "    now = datetime.datetime.now()\n",
    "    print(f'{now}: Checking {url}: ', end='')\n",
    "\n",
    "    # Get soup object from URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Determine if page is a product or collection\n",
    "    products = soup.find_all('li', {'class': 'product'})\n",
    "    collection = len(products) > 0\n",
    "\n",
    "    if collection:\n",
    "        print('collection')\n",
    "        \n",
    "        # Get all unique product links\n",
    "        unique_links = set()\n",
    "        for product in products:\n",
    "            for a in product.find_all('a'):\n",
    "                link = a['href']\n",
    "                if 'http' in link:\n",
    "                    unique_links.add(link)\n",
    "        \n",
    "        # Parse all product links\n",
    "        for link in unique_links:\n",
    "            page_parser(link)\n",
    "\n",
    "    else:\n",
    "        print('product')\n",
    "\n",
    "\n",
    "page_parser(STARTING_LINK)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data From Product Page\n",
    "\n",
    "Now that there is a way to find product pages we need to find a way to scrape all of the following data from the product page:\n",
    "* SKU\n",
    "* Material\n",
    "* Price\n",
    "* Quantity\n",
    "* Color(If applicable)\n",
    "* Wire Gauge\n",
    "* Wire Diameter\n",
    "* Ring Aspect Ratio\n",
    "* Stock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Beautiful Soup\n",
    "\n",
    "As a first start and to learn how to parse values from the page I will use Beautiful Soup to learn how to scrape data From the page.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Sample Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce the number of requests to the website the following code is run to download and store repeatable sample pages for testing.\n",
    "\n",
    "# Determine sample pages\n",
    "product_urls = [\n",
    "    \"https://theringlord.com/enameled-copper-20ga-7-64-id/\",  # Image option select\n",
    "    \"https://theringlord.com/stainless-steel-24g/\",           # radio button option select\n",
    "]\n",
    "\n",
    "# Get Sample pages and store as local variable\n",
    "product_pages = list()\n",
    "for product_url in product_urls:\n",
    "    response = requests.get(product_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_pages.append(soup)\n",
    "    sleep(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enameled copper 20ga 7/64'' ID\n",
      "\t$2.44 USD\n",
      "\t570\n",
      "\t{'imperial': '0.032\"', 'metric': '0.81mm', 'gauge': '20g AWG'}\n",
      "\tEnameled Copper\n",
      "\tSX-EC-20764\n",
      "Stainless Steel 24g\n",
      "\t$4.82 - $4.99 USD\n",
      "\t1000\n",
      "\t{'imperial': '0.02\"', 'metric': '0.5mm', 'gauge': '24g AWG'}\n",
      "\tStainless Steel\n",
      "\tMC-SS-24\n"
     ]
    }
   ],
   "source": [
    "# The purpose of this cell is to test the function I wrote to parse the data on the test pages.\n",
    "\n",
    "\n",
    "# Define a function to parse the product page\n",
    "def parse_product(soup: BeautifulSoup) -> None:\n",
    "\n",
    "    # Get item with product information from page\n",
    "    product_view = soup.find_all('div', {'class': 'productView'})[0]\n",
    "\n",
    "    # Get and clean up title\n",
    "    title = product_view.select('h1[class=productView-title]')[0].get_text()\n",
    "    clean_title = ' '.join([x for x in title.split(' ') if x != ''])\n",
    "\n",
    "    # Print out item title\n",
    "    print(clean_title)\n",
    "\n",
    "    \n",
    "\n",
    "    # Get price currency\n",
    "    currency = soup.find_all('main')[0]['data-currency-code']\n",
    "\n",
    "    # Get price\n",
    "    price = product_view.select('span[data-product-price-without-tax]')[0].get_text()\n",
    "    print(f\"\\t{price} {currency}\")\n",
    "\n",
    "    # Get Quantity\n",
    "    description_list = product_view.find_all('dl')[0]\n",
    "    quantity_description = description_list.find(lambda tag: tag.name == 'dt' and 'Quantity:' in tag.text)\n",
    "    quantity_data = quantity_description.next_sibling.next_sibling\n",
    "    quantity = ''.join([c for c in quantity_data.text if c.isdigit()])\n",
    "\n",
    "    print(f\"\\t{quantity}\")\n",
    "\n",
    "    # Get wire outer diameters\n",
    "    description_list = product_view.find_all('dl')[0]\n",
    "    wire_diameter_description = description_list.find(lambda tag: tag.name == 'dt' and 'Wire OD:' in tag.text)\n",
    "    wire_diameter = wire_diameter_description.next_sibling.next_sibling\n",
    "\n",
    "    wire_text = wire_diameter.text\n",
    "\n",
    "    wds = wire_text.replace('(', '').replace(')', '').replace('= ', '').split(' ')\n",
    "\n",
    "    diameters = {\n",
    "        'imperial': wds[0],\n",
    "        'metric': wds[1],\n",
    "        'gauge': f\"{wds[2]} {wds[3]}\"\n",
    "    }\n",
    "\n",
    "    print(f\"\\t{diameters}\")\n",
    "\n",
    "    # Get ring material\n",
    "    breadcrumb = soup.find_all('nav', attrs={'aria-label': 'Breadcrumb'})[0]\n",
    "    material_link = breadcrumb.find_all('li')[2]\n",
    "    material = material_link.find_all('span')[0].text\n",
    "\n",
    "    print(f'\\t{material}')\n",
    "\n",
    "    # Get SKU\n",
    "    description_list = product_view.find_all('dl')[0]\n",
    "    sku_dt = description_list.find(lambda tag: tag.name == 'dt' and 'SKU:' in tag.text)\n",
    "    sku_tag = sku_dt.next_sibling.next_sibling\n",
    "    sku = sku_tag.text\n",
    "\n",
    "    print(f\"\\t{sku}\")\n",
    "\n",
    "\n",
    "\n",
    "# Run parse function on test products\n",
    "for product_page in product_pages:\n",
    "    parse_product(product_page)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get productView\n",
    "page = product_pages[1]\n",
    "product_view = page.find_all('div', attrs={'class': 'productView'})[0]\n",
    "\n",
    "\n",
    "description_list = product_view.find_all('dl')[0]\n",
    "sku_dt = description_list.find(lambda tag: tag.name == 'dt' and 'SKU:' in tag.text)\n",
    "sku_tag = sku_dt.next_sibling.next_sibling\n",
    "sku = sku_tag.text\n",
    "\n",
    "print(sku)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
